{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d152e813-0505-4f8d-b5c0-c8315a20961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499dcd58-f88d-487e-9341-45994b770b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure NLTK data is available\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Set up logging for debugging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings (optional)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Binary classification\n",
    "model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed152b-5673-4d62-b6c8-1f0fdb5b4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Step 1: Function to summarize content using generative AI ---\n",
    "def summarize_content(statement, content, max_sentences=5, max_length=2000):\n",
    "    \"\"\"Summarize content using ChatGroq, focusing on information relevant to the statement.\"\"\"\n",
    "    try:\n",
    "        # Initialize ChatGroq with the provided API key\n",
    "        api_key = \"Your_Groq_API_Key\"  # Replace with your actual Groq API key\n",
    "        llm = ChatGroq(api_key=api_key, model=\"llama3-8b-8192\")\n",
    "\n",
    "        # Construct a prompt for summarization\n",
    "        prompt_template = PromptTemplate.from_template(\n",
    "            \"\"\"Given the following statement: '{statement}'\n",
    "        And the following content: '{content}'\n",
    "        Summarize the content in up to {max_sentences} sentences, including only information directly relevant to the statement. \n",
    "        Exclude irrelevant details and ensure the summary is coherent, concise, and avoids gibberish. \n",
    "        Limit the summary to {max_length} characters. If no relevant information is found, return 'No relevant summary found'.\"\"\"\n",
    "        )\n",
    "\n",
    "        # Format the prompt with truncated content to avoid excessive length\n",
    "        prompt = prompt_template.format(\n",
    "            statement=statement,\n",
    "            content=content[:10000],  # Truncate content to 10,000 characters as in original\n",
    "            max_sentences=max_sentences,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        # Call ChatGroq to generate the summary\n",
    "        try:\n",
    "            response = llm.invoke(input=prompt)\n",
    "            summary = response.content.strip()\n",
    "\n",
    "            # Validate and truncate summary if necessary\n",
    "            if len(summary) > max_length:\n",
    "                sentences = sent_tokenize(summary)\n",
    "                truncated = \"\"\n",
    "                current_length = 0\n",
    "                for sentence in sentences:\n",
    "                    if current_length + len(sentence) <= max_length:\n",
    "                        truncated += sentence + \" \"\n",
    "                        current_length += len(sentence) + 1\n",
    "                    else:\n",
    "                        break\n",
    "                summary = truncated.strip() or \"No relevant summary found\"\n",
    "\n",
    "            if not summary or summary == \"No relevant summary found\":\n",
    "                logger.warning(\"ChatGroq returned empty or irrelevant summary\")\n",
    "                return fallback_summarize_content(statement, content, max_sentences, max_length)\n",
    "\n",
    "            logger.info(f\"Generated summary for statement '{statement[:60]}...': {summary[:100]}...\")\n",
    "            return summary\n",
    "\n",
    "        except Exception as api_error:\n",
    "            logger.error(f\"ChatGroq API error: {api_error}\")\n",
    "            # Fallback to heuristic-based summarization\n",
    "            return fallback_summarize_content(statement, content, max_sentences, max_length)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error summarizing content: {e}\")\n",
    "        return fallback_summarize_content(statement, content, max_sentences, max_length)\n",
    "\n",
    "# --- Fallback Heuristic-Based Summarization ---\n",
    "def fallback_summarize_content(statement, content, max_sentences=5, max_length=2000):\n",
    "    \"\"\"Fallback heuristic-based summarization if generative AI fails.\"\"\"\n",
    "    try:\n",
    "        # Tokenize content into sentences\n",
    "        sentences = sent_tokenize(content)\n",
    "        if not sentences:\n",
    "            logger.warning(\"No sentences found in content\")\n",
    "            return \"No relevant summary found\"\n",
    "\n",
    "        # Extract keywords from statement, excluding stop words\n",
    "        stop_words = {'a', 'an', 'the', 'is', 'are', 'was', 'were', 'in', 'on', 'at', 'to', 'and', 'or', 'for', 'with', 'by', 'from', 'of'}\n",
    "        statement_words = set(re.findall(r'\\b\\w+\\b', statement.lower())) - stop_words\n",
    "        if not statement_words:\n",
    "            logger.warning(\"No meaningful keywords extracted from statement\")\n",
    "            return \"No relevant summary found\"\n",
    "\n",
    "        # Score sentences based on keyword overlap and relevance\n",
    "        scored_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence_words = set(re.findall(r'\\b\\w+\\b', sentence.lower()))\n",
    "            if not sentence_words:\n",
    "                continue\n",
    "            overlap = len(statement_words.intersection(sentence_words))\n",
    "            word_counts = Counter(sentence_words)\n",
    "            repetition_score = sum(count > 2 for count in word_counts.values())\n",
    "            sentence_length = len(sentence_words)\n",
    "            is_valid = sentence_length > 3 and repetition_score == 0\n",
    "            if overlap > 0 and is_valid:\n",
    "                scored_sentences.append((sentence, overlap))\n",
    "\n",
    "        # Sort by overlap score and select top sentences\n",
    "        scored_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:max_sentences]\n",
    "        summary_sentences = [s[0] for s in scored_sentences]\n",
    "\n",
    "        if not summary_sentences:\n",
    "            logger.warning(\"No relevant sentences found for summary\")\n",
    "            return \"No relevant summary found\"\n",
    "\n",
    "        summary = \" \".join(summary_sentences).strip()\n",
    "        if len(summary) > max_length:\n",
    "            truncated = \"\"\n",
    "            current_length = 0\n",
    "            for sentence in summary_sentences:\n",
    "                if current_length + len(sentence) <= max_length:\n",
    "                    truncated += sentence + \" \"\n",
    "                    current_length += len(sentence) + 1\n",
    "                else:\n",
    "                    break\n",
    "            summary = truncated.strip()\n",
    "\n",
    "        if not summary:\n",
    "            logger.warning(\"Summary is empty after processing\")\n",
    "            return \"No relevant summary found\"\n",
    "\n",
    "        logger.info(f\"Fallback summary for statement '{statement[:60]}...': {summary[:100]}...\")\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in fallback summarization: {e}\")\n",
    "        return \"No relevant summary found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b71cb7-1db6-4ba6-826e-3c37fd6e45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Function to fetch content from a URL ---\n",
    "def get_page_content(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        text_elements = soup.find_all([\"p\", \"div\", \"article\", \"span\"])\n",
    "        text = \" \".join([elem.get_text(strip=True) for elem in text_elements if elem.get_text(strip=True)])\n",
    "        if not text:\n",
    "            logger.warning(f\"No content extracted from {url}\")\n",
    "            return \"No content found\"\n",
    "        return text[:10000]  # Trim to 10000 chars\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching {url}: {e}\")\n",
    "        return f\"Error fetching content: {e}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error for {url}: {e}\")\n",
    "        return f\"Error fetching content: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23218e0-5420-4b1a-8a50-3dcd6ac207af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Use googlesearch to get evidence URLs and content ---\n",
    "def get_search_results(query, max_results=10):\n",
    "    try:\n",
    "        from googlesearch import search\n",
    "        urls = list(search(query, num_results=max_results * 2))\n",
    "        results = []\n",
    "        skipped = 0\n",
    "        for url in urls:\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "            if not url.startswith((\"http://\", \"https://\")):\n",
    "                logger.warning(f\"Skipping invalid URL: {url}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            logger.info(f\"Fetching content from {url}\")\n",
    "            content = get_page_content(url)\n",
    "            if content.startswith(\"Error\") or content == \"No content found\":\n",
    "                logger.info(f\"Skipping {url} due to {'error' if content.startswith('Error') else 'empty content'}\")\n",
    "                skipped += 1\n",
    "            else:\n",
    "                results.append({\"url\": url, \"content\": content})\n",
    "            time.sleep(5)\n",
    "        logger.info(f\"Skipped {skipped} URLs, fetched {len(results)} valid results for query: {query[:60]}...\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Search error for query '{query[:60]}...': {e}\")\n",
    "        return [{\"url\": None, \"content\": f\"Search error: {e}\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "080751fa-cc98-4447-8526-c7b868ed8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Adjust prediction using BERT ---\n",
    "def adjust_prediction_with_external_evidence(statement, evidence_data):\n",
    "    \"\"\"Use BERT to predict truthfulness based on statement and summarized evidence\"\"\"\n",
    "    summaries = [\n",
    "        summarize_content(statement, item[\"content\"])\n",
    "        for item in evidence_data\n",
    "        if not item[\"content\"].startswith(\"Error\") and item[\"content\"] != \"No content found\"\n",
    "    ]\n",
    "\n",
    "    if not summaries:\n",
    "        logger.warning(f\"No valid summaries found for statement: {statement[:60]}...\")\n",
    "        return 0.0, []  # Return 0.0 probability and empty scores list\n",
    "\n",
    "    # Combine statement with each summarized evidence text\n",
    "    probabilities = []\n",
    "    summaries_lst = []\n",
    "    for summary in summaries:\n",
    "        if summary == \"No relevant summary found\":\n",
    "            continue\n",
    "        input_text = f\"[CLS] {statement} [SEP] {summary} [SEP]\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1).detach().numpy()[0]\n",
    "            true_prob = float(probs[1])  # Probability of \"true\" class\n",
    "            probabilities.append(true_prob)\n",
    "            summaries_lst.append(summary)\n",
    "    \n",
    "    # Use max probability to avoid diluting strong signals\n",
    "    final_prob = max(probabilities) if probabilities else 0.0\n",
    "    logger.info(f\"Adjusted probability for statement '{statement[:60]}...': {final_prob:.4f}\")\n",
    "    logger.info(f\"Evidence probabilities: {probabilities}\")\n",
    "    return final_prob, probabilities, summaries_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b641ea24-5062-4184-92db-f447405adb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Apply adjustment over DataFrame ---\n",
    "TEST_PATH = \"liar_dataset/test.tsv\"\n",
    "try:\n",
    "    test_df = pd.read_csv(TEST_PATH, sep='\\t')\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"File {TEST_PATH} not found.\")\n",
    "    exit(1)\n",
    "\n",
    "# Verify column for statements\n",
    "statement_col = 'statement' if 'statement' in test_df.columns else 2\n",
    "\n",
    "# Sample 5 random rows\n",
    "random_indices = random.sample(range(len(test_df)), 1)\n",
    "sample_df = test_df.iloc[random_indices].copy()\n",
    "\n",
    "# Initialize lists to store results\n",
    "adjusted_probs = []\n",
    "web_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c4398ba-6238-455c-a758-398992f154b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process sample\n",
    "# for counter, (i, row) in enumerate(sample_df.iterrows(), 1):\n",
    "#     statement = row[statement_col] if isinstance(statement_col, str) else row.iloc[statement_col]\n",
    "#     logger.info(f\"Processing statement {counter}/{len(sample_df)}: {statement[:60]}...\")\n",
    "    \n",
    "#     # Fetch search results\n",
    "#     search_results = get_search_results(statement)\n",
    "    \n",
    "#     # Calculate adjusted probability\n",
    "#     if search_results:\n",
    "#         adjusted_prob, evidence_probs = adjust_prediction_with_external_evidence(statement, search_results)\n",
    "#         print(f\"Statement {counter}: {statement[:60]}...\")\n",
    "#         print(f\"Evidence probabilities: {evidence_probs}\")\n",
    "#         adjusted_probs.append(adjusted_prob)\n",
    "#     else:\n",
    "#         logger.warning(f\"No search results for statement: {statement[:60]}...\")\n",
    "#         adjusted_probs.append(0.0)\n",
    "#         evidence_probs = []\n",
    "\n",
    "#     # Store web data with summaries and probabilities\n",
    "#     valid_result_index = 0\n",
    "#     for result in search_results:\n",
    "#         original_content = result[\"content\"]\n",
    "#         summary = summarize_content(statement, original_content) if not original_content.startswith(\"Error\") and original_content != \"No content found\" else original_content\n",
    "#         prob = evidence_probs[valid_result_index] if valid_result_index < len(evidence_probs) and summary != \"No relevant summary found\" else None\n",
    "#         web_data.append({\n",
    "#             \"statement\": statement,\n",
    "#             \"url\": result[\"url\"],\n",
    "#             \"content\": original_content,\n",
    "#             \"content_summary\": summary,\n",
    "#             \"probability\": prob\n",
    "#         })\n",
    "#         if summary != \"No relevant summary found\" and not original_content.startswith(\"Error\") and original_content != \"No content found\":\n",
    "#             valid_result_index += 1\n",
    "\n",
    "# # Create DataFrame for web data\n",
    "# web_df = pd.DataFrame(web_data)\n",
    "\n",
    "# # Define threshold\n",
    "# BEST_THRESHOLD = 0.6\n",
    "\n",
    "# # Update sample DataFrame with adjusted results\n",
    "# sample_df[\"Adjusted Final Probability\"] = [f\"{p:.4f}\" for p in adjusted_probs]\n",
    "# sample_df[\"Adjusted Final Class\"] = ['true' if p > BEST_THRESHOLD else 'false' for p in adjusted_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8b38e-3c37-4037-9bcd-bd3b2f938d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 16:36:06,400 - INFO - Processing statement 1/1: One man sacrificed for his country. One man opposed a flawed...\n",
      "2025-07-23 16:36:07,328 - INFO - Fetching content from https://www.politifact.com/factchecks/2008/feb/07/john-mccain/one-man-found-his-winning-message/\n",
      "2025-07-23 16:36:12,531 - INFO - Fetching content from https://www.cbsnews.com/news/mccain-stresses-message-in-sc-ad/\n",
      "2025-07-23 16:36:18,197 - INFO - Fetching content from https://www.nytimes.com/2007/09/28/us/politics/28adbox.html\n",
      "2025-07-23 16:36:18,270 - ERROR - Error fetching https://www.nytimes.com/2007/09/28/us/politics/28adbox.html: 403 Client Error: Forbidden for url: https://www.nytimes.com/2007/09/28/us/politics/28adbox.html\n",
      "2025-07-23 16:36:18,271 - INFO - Skipping https://www.nytimes.com/2007/09/28/us/politics/28adbox.html due to error\n",
      "2025-07-23 16:36:23,278 - INFO - Fetching content from https://www.politifact.com/factchecks/2007/oct/02/john-mccain/he-told-you-so-/\n",
      "2025-07-23 16:36:28,569 - INFO - Fetching content from https://www.cbsnews.com/news/mccain-to-spotlight-pow-experience-in-ads/\n",
      "2025-07-23 16:36:34,286 - INFO - Fetching content from https://thehill.com/homenews/campaign/760-mccain-hits-the-airwaves/\n",
      "2025-07-23 16:36:34,409 - ERROR - Error fetching https://thehill.com/homenews/campaign/760-mccain-hits-the-airwaves/: 403 Client Error: Forbidden for url: https://thehill.com/homenews/campaign/760-mccain-hits-the-airwaves/\n",
      "2025-07-23 16:36:34,410 - INFO - Skipping https://thehill.com/homenews/campaign/760-mccain-hits-the-airwaves/ due to error\n",
      "2025-07-23 16:36:39,415 - INFO - Fetching content from https://p2008.org/ads08/mccainad092707om.html\n",
      "2025-07-23 16:36:44,957 - INFO - Fetching content from https://www.chicagotribune.com/2007/12/26/john-wayne-mccains-new-ad-never-surrender/\n",
      "2025-07-23 16:36:45,293 - ERROR - Error fetching https://www.chicagotribune.com/2007/12/26/john-wayne-mccains-new-ad-never-surrender/: 403 Client Error: Forbidden for url: https://www.chicagotribune.com/2007/12/26/john-wayne-mccains-new-ad-never-surrender/\n",
      "2025-07-23 16:36:45,294 - INFO - Skipping https://www.chicagotribune.com/2007/12/26/john-wayne-mccains-new-ad-never-surrender/ due to error\n",
      "2025-07-23 16:36:50,301 - INFO - Fetching content from https://p2008.org/ads08/mccainad122607.html\n",
      "2025-07-23 16:36:55,804 - INFO - Fetching content from https://www.postandcourier.com/politics/adwatch-mccain-focuses-on-war-record-in-latest-south-carolina-ad/article_64860d7d-d7c5-5cb3-8b55-e2884f0f0b30.html\n",
      "2025-07-23 16:37:01,674 - INFO - Fetching content from https://dumas.ccsd.cnrs.fr/dumas-04324401/document\n",
      "2025-07-23 16:37:02,979 - WARNING - Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "2025-07-23 16:37:02,996 - WARNING - No content extracted from https://dumas.ccsd.cnrs.fr/dumas-04324401/document\n",
      "2025-07-23 16:37:03,011 - INFO - Skipping https://dumas.ccsd.cnrs.fr/dumas-04324401/document due to empty content\n",
      "2025-07-23 16:37:08,017 - INFO - Fetching content from https://www.govinfo.gov/content/pkg/CRECB-2007-pt3/html/CRECB-2007-pt3-Pg4455-8.htm\n",
      "2025-07-23 16:37:09,421 - WARNING - No content extracted from https://www.govinfo.gov/content/pkg/CRECB-2007-pt3/html/CRECB-2007-pt3-Pg4455-8.htm\n",
      "2025-07-23 16:37:09,422 - INFO - Skipping https://www.govinfo.gov/content/pkg/CRECB-2007-pt3/html/CRECB-2007-pt3-Pg4455-8.htm due to empty content\n",
      "2025-07-23 16:37:14,427 - INFO - Fetching content from https://press.un.org/en/2024/ga12627.doc.htm\n",
      "2025-07-23 16:37:20,626 - INFO - Fetching content from https://www.govinfo.gov/content/pkg/CREC-2007-02-13/html/CREC-2007-02-13-pt1-PgH1492-2.htm\n",
      "2025-07-23 16:37:23,330 - WARNING - No content extracted from https://www.govinfo.gov/content/pkg/CREC-2007-02-13/html/CREC-2007-02-13-pt1-PgH1492-2.htm\n",
      "2025-07-23 16:37:23,331 - INFO - Skipping https://www.govinfo.gov/content/pkg/CREC-2007-02-13/html/CREC-2007-02-13-pt1-PgH1492-2.htm due to empty content\n",
      "2025-07-23 16:37:28,332 - INFO - Fetching content from https://www.nbcnews.com/id/wbna6969478\n",
      "2025-07-23 16:37:34,195 - INFO - Fetching content from https://www.theyworkforyou.com/debates/?id=2024-10-28f.586.0\n"
     ]
    }
   ],
   "source": [
    "for counter, (i, row) in enumerate(sample_df.iterrows(), 1):\n",
    "    statement = row[statement_col] if isinstance(statement_col, str) else row.iloc[statement_col]\n",
    "    logger.info(f\"Processing statement {counter}/{len(sample_df)}: {statement[:60]}...\")\n",
    "    \n",
    "    # Fetch search results\n",
    "    search_results = get_search_results(statement)\n",
    "    \n",
    "    # Calculate adjusted probability\n",
    "    if search_results:\n",
    "        adjusted_prob, evidence_probs, summaries_lst = adjust_prediction_with_external_evidence(statement, search_results)\n",
    "        print(f\"Statement {counter}: {statement[:60]}...\")\n",
    "        print(f\"Evidence probabilities: {evidence_probs}\")\n",
    "        adjusted_probs.append(adjusted_prob)\n",
    "    else:\n",
    "        logger.warning(f\"No search results for statement: {statement[:60]}...\")\n",
    "        adjusted_probs.append(0.0)\n",
    "        evidence_probs = []\n",
    "        summaries_lst = []  # Ensure summaries_lst is defined even if no search results\n",
    "\n",
    "    # Store web data with summaries and probabilities\n",
    "    valid_result_index = 0\n",
    "    for idx, result in enumerate(search_results):\n",
    "        original_content = result[\"content\"]\n",
    "        # Use summary from summaries_lst if available, otherwise use original_content or fallback\n",
    "        summary = summaries_lst[idx] if idx < len(summaries_lst) and summaries_lst[idx] not in [\"Error\", \"No content found\", \"No relevant summary found\"] else original_content\n",
    "        prob = evidence_probs[valid_result_index] if valid_result_index < len(evidence_probs) and summary != \"No relevant summary found\" else None\n",
    "        web_data.append({\n",
    "            \"statement\": statement,\n",
    "            \"url\": result[\"url\"],\n",
    "            \"content\": original_content,\n",
    "            \"content_summary\": summary,\n",
    "            \"probability\": prob\n",
    "        })\n",
    "        if summary != \"No relevant summary found\" and not original_content.startswith(\"Error\") and original_content != \"No content found\":\n",
    "            valid_result_index += 1\n",
    "\n",
    "# Create DataFrame for web data\n",
    "web_df = pd.DataFrame(web_data)\n",
    "\n",
    "# Define threshold\n",
    "BEST_THRESHOLD = 0.6\n",
    "\n",
    "# Update sample DataFrame with adjusted results\n",
    "sample_df[\"Adjusted Final Probability\"] = [f\"{p:.4f}\" for p in adjusted_probs]\n",
    "sample_df[\"Adjusted Final Class\"] = ['true' if p > BEST_THRESHOLD else 'false' for p in adjusted_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5a87e-e85b-48f6-b079-bc1cfcf60ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7bebc-1d3c-4adf-b57b-45793667f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52032f37-21c6-4268-8c7a-050d8512ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_df['content'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f360ea2c-1ba3-4dbb-b575-7ff5322fa294",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_df['content_summary'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f73af0-f441-4c33-8381-935a9d84da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_df['statement'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24ba76-8704-46b2-864d-f674df79f324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc933f-431e-49e0-b424-923f4f8d4531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
